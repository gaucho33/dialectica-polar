{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c5b04-c1b4-47c5-9914-78eae9d741a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Explicación del Código y Cómo Usarlo en Jupyter:\n",
    "\n",
    "Entorno Jupyter: El código está escrito para ser ejecutado en celdas de un Jupyter Notebook. \n",
    "La instalación de librerías (!pip install) se realiza en una celda inicial.\n",
    "Cargar y Preprocesar MNIST: Igual que antes, carga el dataset MNIST y lo preprocesa (normalización y expansión de dimensiones).\n",
    "\n",
    "Modelo CNN y Embedding: Se define y entrena un modelo CNN temporal para obtener embeddings significativos. \n",
    "Luego se extrae la parte de extracción de embeddings como embedding_model.\n",
    "\n",
    "También obtenemos los pesos (final_layer_weights) de la capa final de clasificación del modelo temporal entrenado.\n",
    "Función compute_matrices_from_vector: Similar a la anterior, pero toma un único vector (no dos embeddings) \n",
    "y calcula las matrices Contenido, Continente y Resultado a partir de su producto exterior consigo mismo.\n",
    "\n",
    "Funciones Auxiliares: flatten_matrix y get_matrix_scalar son las mismas que antes.\n",
    "Calcular Matrices por Identidad (Sección 9):\n",
    "Iteramos a través de cada clase (0 a 9).\n",
    "Para cada clase, obtenemos el vector de pesos correspondiente de final_layer_weights.\n",
    "\n",
    "Usamos compute_matrices_from_vector para obtener las matrices C k​,Co k​,R k​ para esta clase.\n",
    "Aplanamos C k​ y R k​ y los almacenamos en diccionarios (flattened_content_vectors_per_class, flattened_result_vectors_per_class) \n",
    "indexados por el ID de la clase. Ahora tenemos un \"vector template\" aplanado para Contenido y Resultado por cada dígito.\n",
    "Procesar Conjunto de Prueba (Sección 10):\n",
    "Obtenemos los embeddings para todas las imágenes del conjunto de prueba.\n",
    "\n",
    "Iteramos sobre cada embedding de prueba:\n",
    "Calculamos la matriz de Interacción de la imagen actual (e img​ ⊗e img​ ) \n",
    "y la aplanamos (flattened_image_interaction_vector). \n",
    "Este vector representa el \"patrón\" de la imagen actual en el espacio de embedding.\n",
    "\n",
    "Nuevo: Escalar de la Matriz Resultado de la Imagen: \n",
    "Calculamos la matriz Resultado para la imagen actual (usando compute_matrices_from_vector con el embedding de la imagen) \n",
    "y obtenemos su escalar usando get_matrix_scalar. Este escalar se guarda para mostrarlo al final.\n",
    "\n",
    "Clasificación: Para clasificar la imagen actual, iteramos a través de cada clase (0 a 9):\n",
    "Obtenemos el vector aplanado de Contenido de la clase k (flattened_content_vectors_per_class[k]) \n",
    "y el vector aplanado de Resultado de la clase k.\n",
    "Calculamos un score de similitud entre el vector de interacción de la imagen (flattened_image_interaction_vector) \n",
    "y el vector aplanado de Contenido de la clase k (usando np.dot). Hacemos lo mismo para Resultado.\n",
    "Guardamos los 10 scores de Contenido y 10 scores de Resultado para esta imagen.\n",
    "\n",
    "Después de calcular los scores para las 10 clases, la predicción para Contenido es la clase con el score de Contenido más alto (np.argmax).\n",
    "Lo mismo para Resultado.\n",
    "Las predicciones se guardan en listas.\n",
    "\n",
    "Calcular y Mostrar Aciertos (Sección 11): Se calcula el accuracy_score comparando las predicciones basadas en Contenido y Resultado \n",
    "con las etiquetas verdaderas del conjunto de prueba (y_test).\n",
    "\n",
    "Mostrar Escalar de Resultado (Sección 12): Se calcula la media de todos los escalares de Resultado \n",
    "que se calcularon para cada imagen de prueba y se muestra este valor promedio.\n",
    "\n",
    "Para ejecutarlo en Jupyter, Este programa implementa las definiciones de matrices por identidad \n",
    "y las utiliza para la clasificación comparando la representación de la imagen con la representación de cada identidad.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606f802b-437d-4933-b862-1734abd09e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\2016a\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\2016a\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\2016a\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\2016a\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\2016a\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\2016a\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow numpy scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4126f47d-704c-41bd-b45d-f0dc1b612c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, using CPU\n"
     ]
    }
   ],
   "source": [
    "# 2. Importar librerías\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # Para barras de progreso en Jupyter\n",
    "\n",
    "# Optional: Configure TensorFlow to use GPU if available\n",
    "# (Less critical for MNIST, but good practice)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96e7f5d-34e9-45fc-9c7d-8229ca5f3316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando y preprocesando dataset MNIST...\n",
      "Dataset MNIST cargado y preprocesado.\n",
      "Forma de datos de entrenamiento: (60000, 28, 28, 1)\n",
      "Forma de etiquetas de entrenamiento: (60000,)\n",
      "Forma de datos de prueba: (10000, 28, 28, 1)\n",
      "Forma de etiquetas de prueba: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 3. Cargar y preprocesar el dataset MNIST\n",
    "print(\"Cargando y preprocesando dataset MNIST...\")\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizar las imágenes a valores entre 0 y 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Expandir las dimensiones para que tengan 1 canal (escala de grises)\n",
    "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\n",
    "\n",
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Las etiquetas y_train y y_test ya son enteros (0-9)\n",
    "num_classes = 10\n",
    "\n",
    "print(\"Dataset MNIST cargado y preprocesado.\")\n",
    "print(f\"Forma de datos de entrenamiento: {x_train.shape}\")\n",
    "print(f\"Forma de etiquetas de entrenamiento: {y_train.shape}\")\n",
    "print(f\"Forma de datos de prueba: {x_test.shape}\")\n",
    "print(f\"Forma de etiquetas de prueba: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd8513-2d96-4357-900b-42aa78d70b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Definir el modelo CNN para extracción de características\n",
    "def create_cnn_embedding_model(input_shape, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Define un modelo CNN que extrae un embedding.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=input_shape, name='input_image')\n",
    "\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Capa densa para obtener el embedding (vector de características)\n",
    "    embedding_layer = Dense(embedding_dim, activation='relu', name='embedding_output')(x)\n",
    "\n",
    "    # Modelo solo para extraer embeddings\n",
    "    embedding_model = Model(inputs=input_layer, outputs=embedding_layer, name='mnist_embedding_extractor')\n",
    "\n",
    "    return embedding_model, embedding_layer.output_shape[-1] # Return model and embedding dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7856c82-bb5a-4a26-a6ed-518c62b091d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo temporal para obtener embeddings significativos...\n",
      "Epoch 1/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 935ms/step - accuracy: 0.8337 - loss: 0.5738 - val_accuracy: 0.9761 - val_loss: 0.0806\n",
      "Epoch 2/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 768ms/step - accuracy: 0.9786 - loss: 0.0683 - val_accuracy: 0.9842 - val_loss: 0.0523\n",
      "Epoch 3/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 597ms/step - accuracy: 0.9859 - loss: 0.0462 - val_accuracy: 0.9880 - val_loss: 0.0425\n",
      "Epoch 4/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 667ms/step - accuracy: 0.9899 - loss: 0.0331 - val_accuracy: 0.9870 - val_loss: 0.0464\n",
      "Epoch 5/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 809ms/step - accuracy: 0.9923 - loss: 0.0246 - val_accuracy: 0.9883 - val_loss: 0.0451\n",
      "\n",
      "Modelo extractor de embeddings creado.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"final_embedding_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"final_embedding_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_model_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cnn_to_flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ temp_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_model_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cnn_to_flatten (\u001b[38;5;33mSequential\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │        \u001b[38;5;34m18,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ temp_embedding (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,744</span> (874.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m223,744\u001b[0m (874.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,744</span> (874.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m223,744\u001b[0m (874.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa final de clasificación obtenidos. Forma: (128, 10)\n"
     ]
    }
   ],
   "source": [
    "# 5. Definir y entrenar el modelo extractor de embeddings (temporalmente con clasificador final)\n",
    "EMBEDDING_DIM = 128 # Tamaño del vector de características que queremos\n",
    "\n",
    "# Necesitamos entrenar la CNN para que los embeddings sean significativos\n",
    "# Para entrenar la CNN, temporalmente añadimos una capa de clasificación final\n",
    "# y luego la descartamos.\n",
    "\n",
    "temp_full_model_input = Input(shape=input_shape, name='full_model_input')\n",
    "# Creamos el Sequential model para la CNN + embedding\n",
    "temp_cnn_embedding_seq = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(EMBEDDING_DIM, activation='relu', name='temp_embedding')\n",
    "], name='my_cnn_sequence') # Le damos un nombre explícito\n",
    "\n",
    "temp_embedding_output_tensor = temp_cnn_embedding_seq(temp_full_model_input)\n",
    "temp_classifier_output = Dense(num_classes, activation='softmax', name='temp_classifier_output')(temp_embedding_output_tensor)\n",
    "temp_full_model = Model(inputs=temp_full_model_input, outputs=temp_classifier_output)\n",
    "\n",
    "temp_full_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                        optimizer=Adam(learning_rate=0.001),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nEntrenando modelo temporal para obtener embeddings significativos...\")\n",
    "history = temp_full_model.fit(x_train, y_train,\n",
    "                              batch_size=128,\n",
    "                              epochs=5, # Entrenar por 5 épocas\n",
    "                              verbose=1,\n",
    "                              validation_split=0.2)\n",
    "\n",
    "# Ahora creamos el modelo extractor de embeddings a partir del modelo entrenado\n",
    "# Acceder a la capa Sequential por su nombre explícito\n",
    "temp_sequential_model_instance = temp_full_model.get_layer('my_cnn_sequence')\n",
    "\n",
    "# Acceder a la capa Dense de embedding dentro del Sequential por su nombre\n",
    "embedding_dense_layer_instance = temp_sequential_model_instance.get_layer('temp_embedding')\n",
    "\n",
    "# Definir la entrada para el NUEVO modelo de embedding (misma forma que el original)\n",
    "new_embedding_input = Input(shape=input_shape, name='embedding_model_input')\n",
    "\n",
    "# Conectar la nueva entrada a las capas de la CNN + Flatten dentro del Sequential (excluyendo la última Dense)\n",
    "# Creamos un Sequential temporal con las capas hasta Flatten\n",
    "cnn_to_flatten_sequence = Sequential(temp_sequential_model_instance.layers[:-1], name='cnn_to_flatten')\n",
    "cnn_features_tensor = cnn_to_flatten_sequence(new_embedding_input)\n",
    "\n",
    "# Conectar los features aplanados a la capa Dense de embedding original\n",
    "embedding_output_tensor = embedding_dense_layer_instance(cnn_features_tensor)\n",
    "\n",
    "# Definir el modelo final de embedding extractor\n",
    "embedding_model = Model(inputs=new_embedding_input, outputs=embedding_output_tensor, name='final_embedding_model')\n",
    "\n",
    "\n",
    "print(\"\\nModelo extractor de embeddings creado.\")\n",
    "embedding_model.summary()\n",
    "\n",
    "# Obtener los pesos de la capa final de clasificación entrenada ( Dense(10) )\n",
    "final_layer_weights, final_layer_biases = temp_full_model.get_layer('temp_classifier_output').get_weights()\n",
    "print(f\"\\nPesos de la capa final de clasificación obtenidos. Forma: {final_layer_weights.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a5ad59-9f45-4026-a7dc-e8fa2fa22a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Función para calcular matrices (Contenido, Continente, Resultado) desde un vector\n",
    "def compute_matrices_from_vector(vector, threshold_mask_quantile=0.9):\n",
    "    \"\"\"\n",
    "    Calcula las matrices de Contenido, Continente y Resultado\n",
    "    a partir de un único vector (usando outer product consigo mismo).\n",
    "    \"\"\"\n",
    "    # Asegurarse de que es un vector 1D\n",
    "    vector = vector.flatten()\n",
    "\n",
    "    # Matriz de Interacción A: producto exterior del vector consigo mismo\n",
    "    interaction_matrix = np.outer(vector, vector)\n",
    "\n",
    "    # Determinar el umbral para la máscara\n",
    "    abs_interaction = np.abs(interaction_matrix)\n",
    "    if np.all(abs_interaction == 0):\n",
    "        threshold_value = 0\n",
    "    else:\n",
    "        threshold_value = np.quantile(abs_interaction.flatten(), threshold_mask_quantile)\n",
    "\n",
    "    # Máscara M: 1 si el valor absoluto está por encima/igual del umbral, 0 de lo contrario\n",
    "    mask = (abs_interaction >= threshold_value).astype(float)\n",
    "\n",
    "    # Matriz de Contenido: A * M (element-wise)\n",
    "    content_matrix = interaction_matrix * mask\n",
    "\n",
    "    # Matriz de Continente: A * (1 - M) (element-wise)\n",
    "    continent_matrix = interaction_matrix * (1 - mask)\n",
    "\n",
    "    # Matriz de Resultado: Continente / Contenido\n",
    "    epsilon = 1e-8\n",
    "    denominator_c = content_matrix.copy()\n",
    "    denominator_c[np.abs(denominator_c) < epsilon] = epsilon\n",
    "    result_matrix = continent_matrix / denominator_c\n",
    "\n",
    "    return content_matrix, continent_matrix, result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d529525f-c407-407d-94d3-918d8e676237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Función para aplanar una matriz a un vector\n",
    "def flatten_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Aplanar una matriz a un vector 1D.\n",
    "    \"\"\"\n",
    "    return matrix.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac948ec-3a6b-4d06-b395-8f71bfa4d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Función para convertir una matriz en un escalar\n",
    "def get_matrix_scalar(matrix):\n",
    "    \"\"\"\n",
    "    Convierte una matriz en un único valor escalar (suma de valores absolutos).\n",
    "    \"\"\"\n",
    "    # Manejar posibles NaN o Inf\n",
    "    return np.sum(np.abs(matrix[np.isfinite(matrix)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a4edb2-1e89-4413-8bf8-51675ce376d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando matrices de atención por identidad (0-9)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b876d29c76b427d92dbf334a737600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices de atención por identidad calculadas y aplanadas.\n"
     ]
    }
   ],
   "source": [
    "# 9. Calcular las 10 matrices (Contenido, Continente, Resultado) por identidad\n",
    "print(\"\\nCalculando matrices de atención por identidad (0-9)...\")\n",
    "\n",
    "# Almacenaremos los vectores aplanados de las matrices Contenido y Resultado para cada clase\n",
    "flattened_content_vectors_per_class = {}\n",
    "flattened_result_vectors_per_class = {}\n",
    "\n",
    "# Cuantil para la máscara (aplicado a la matriz de interacción de cada vector de peso de clase)\n",
    "mask_threshold_quantile = 0.9\n",
    "\n",
    "# Iterar sobre cada clase (0 a 9)\n",
    "for class_id in tqdm(range(num_classes)):\n",
    "    # Obtener el vector de pesos de la capa final correspondiente a esta clase\n",
    "    class_weight_vector = final_layer_weights[:, class_id]\n",
    "\n",
    "    # Calcular las matrices de atención para este vector de pesos de clase\n",
    "    content_m, continent_m, result_m = compute_matrices_from_vector(class_weight_vector, mask_threshold_quantile)\n",
    "\n",
    "    # Almacenar los vectores aplanados de las matrices de Contenido y Resultado\n",
    "    flattened_content_vectors_per_class[class_id] = flatten_matrix(content_m)\n",
    "    flattened_result_vectors_per_class[class_id] = flatten_matrix(result_m)\n",
    "\n",
    "print(\"Matrices de atención por identidad calculadas y aplanadas.\")\n",
    "# print(f\"Forma del vector aplanado por clase: {flattened_content_vectors_per_class[0].shape}\") # Debería ser (EMBEDDING_DIM * EMBEDDING_DIM,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa00539-654e-4e0a-a5e7-d4c868ad5a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de prueba y obteniendo predicciones...\n",
      "Obtenidos 10000 embeddings de prueba.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1704b6b978d849c7b24cd435b35fc97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento del conjunto de prueba completado.\n"
     ]
    }
   ],
   "source": [
    "# 10. Procesar el conjunto de prueba, calcular scores y obtener predicciones\n",
    "print(\"\\nProcesando conjunto de prueba y obteniendo predicciones...\")\n",
    "\n",
    "predictions_c = [] # Predicciones basadas en matrices de Contenido\n",
    "predictions_r = [] # Predicciones basadas en matrices de Resultado\n",
    "result_scalars_test_images = [] # Escalares de la matriz Resultado *de cada imagen de prueba*\n",
    "\n",
    "# Obtener embeddings para todo el conjunto de prueba\n",
    "test_embeddings = embedding_model.predict(x_test, verbose=0)\n",
    "print(f\"Obtenidos {len(test_embeddings)} embeddings de prueba.\")\n",
    "\n",
    "# Iterar sobre cada imagen de prueba\n",
    "for i, embedding in tqdm(enumerate(test_embeddings), total=len(test_embeddings)):\n",
    "    # true_label = y_test[i] # Etiqueta verdadera de esta imagen\n",
    "\n",
    "    # --- Calcular la matriz de Interacción *de la imagen* y aplanarla ---\n",
    "    # Esto representa las interacciones dentro del embedding de la imagen\n",
    "    image_interaction_matrix = np.outer(embedding, embedding)\n",
    "    flattened_image_interaction_vector = flatten_matrix(image_interaction_matrix)\n",
    "\n",
    "    # --- Calcular el escalar de la Matriz de Resultado *de esta imagen* ---\n",
    "    # Esto es un requisito separado: obtener el escalar de la matriz Resultado DE LA IMAGEN\n",
    "    _, _, result_matrix_this_image = compute_matrices_from_vector(embedding, mask_threshold_quantile) # Usar el embedding de la imagen\n",
    "    scalar_r_this_image = get_matrix_scalar(result_matrix_this_image)\n",
    "    result_scalars_test_images.append(scalar_r_this_image)\n",
    "\n",
    "\n",
    "    # --- Calcular scores de clasificación usando matrices POR IDENTIDAD ---\n",
    "    # Para cada imagen, calculamos un score para cada clase (0-9)\n",
    "\n",
    "    scores_c_for_image = [] # Scores usando matrices de Contenido por clase\n",
    "    scores_r_for_image = [] # Scores usando matrices de Resultado por clase\n",
    "\n",
    "    # Iterar sobre cada clase (0 a 9) para calcular el score con la imagen actual\n",
    "    for class_id in range(num_classes):\n",
    "        # Obtener los vectores aplanados de las matrices de Contenido y Resultado para ESTA clase\n",
    "        class_content_vector = flattened_content_vectors_per_class[class_id]\n",
    "        class_result_vector = flattened_result_vectors_per_class[class_id]\n",
    "\n",
    "        # Calcular el score: dot product entre el vector de interacción de la imagen\n",
    "        # y el vector aplanado de la matriz de la clase.\n",
    "        # Asegurarnos de que los vectores aplanados de clase no tienen NaN/Inf antes del dot product\n",
    "        # (Esto debería haber sido manejado en compute_matrices_from_vector si es necesario,\n",
    "        # o nan_to_num aplicado a flattened_content_vectors_per_class/result_vectors_per_class\n",
    "        # después de la sección 9). Agregaremos nan_to_num por seguridad aquí.\n",
    "        class_content_vector_finite = np.nan_to_num(class_content_vector, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        class_result_vector_finite = np.nan_to_num(class_result_vector, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        image_interaction_vector_finite = np.nan_to_num(flattened_image_interaction_vector, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "\n",
    "        score_c_k = np.dot(image_interaction_vector_finite, class_content_vector_finite)\n",
    "        score_r_k = np.dot(image_interaction_vector_finite, class_result_vector_finite)\n",
    "\n",
    "        scores_c_for_image.append(score_c_k)\n",
    "        scores_r_for_image.append(score_r_k)\n",
    "\n",
    "    # --- Obtener la clase predicha para esta imagen ---\n",
    "    # La clase predicha es aquella con el score más alto\n",
    "    predicted_class_c = np.argmax(scores_c_for_image)\n",
    "    predicted_class_r = np.argmax(scores_r_for_image)\n",
    "\n",
    "    # Añadir predicciones a las listas\n",
    "    predictions_c.append(predicted_class_c)\n",
    "    predictions_r.append(predicted_class_r)\n",
    "\n",
    "\n",
    "predictions_c = np.array(predictions_c)\n",
    "predictions_r = np.array(predictions_r)\n",
    "result_scalars_test_images = np.array(result_scalars_test_images) # Convertir a array numpy\n",
    "\n",
    "print(\"Procesamiento del conjunto de prueba completado.\")\n",
    "# print(f\"Predicciones Contenido (primeras 10): {predictions_c[:10]}\")\n",
    "# print(f\"Predicciones Resultado (primeras 10): {predictions_r[:10]}\")\n",
    "# print(f\"Etiquetas verdaderas (primeras 10): {y_test[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3039ac-ed8c-4730-88fa-aed6bf435bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de prueba y obteniendo predicciones...\n",
      "Obtenidos 10000 embeddings de prueba.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7440dd2d414bc1a6332a87102c7849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento del conjunto de prueba completado.\n",
      "\n",
      "--- Porcentajes de Acierto en el Conjunto de Prueba ---\n",
      "Acierto usando Matrices de Contenido (por identidad): 42.81%\n",
      "Acierto usando Matrices de Resultado (por identidad): 95.68%\n"
     ]
    }
   ],
   "source": [
    "# 10. Procesar el conjunto de prueba, calcular scores y obtener predicciones\n",
    "print(\"\\nProcesando conjunto de prueba y obteniendo predicciones...\")\n",
    "\n",
    "predictions_c = [] # Predicciones basadas en matrices de Contenido\n",
    "predictions_r = [] # Predicciones basadas en matrices de Resultado\n",
    "result_scalars_test_images = [] # Escalares de la matriz Resultado *de cada imagen de prueba*\n",
    "\n",
    "# Obtener embeddings para todo el conjunto de prueba\n",
    "test_embeddings = embedding_model.predict(x_test, verbose=0)\n",
    "print(f\"Obtenidos {len(test_embeddings)} embeddings de prueba.\")\n",
    "\n",
    "# Iterar sobre cada imagen de prueba\n",
    "for i, embedding in tqdm(enumerate(test_embeddings), total=len(test_embeddings)):\n",
    "    # true_label = y_test[i] # Etiqueta verdadera de esta imagen\n",
    "\n",
    "    # --- Calcular la matriz de Interacción *de la imagen* y aplanarla ---\n",
    "    # Esto representa las interacciones dentro del embedding de la imagen\n",
    "    image_interaction_matrix = np.outer(embedding, embedding)\n",
    "    flattened_image_interaction_vector = flatten_matrix(image_interaction_matrix)\n",
    "\n",
    "    # --- Calcular el escalar de la Matriz de Resultado *de esta imagen* ---\n",
    "    # Esto es un requisito separado: obtener el escalar de la matriz Resultado DE LA IMAGEN\n",
    "    _, _, result_matrix_this_image = compute_matrices_from_vector(embedding, mask_threshold_quantile) # Usar el embedding de la imagen\n",
    "    scalar_r_this_image = get_matrix_scalar(result_matrix_this_image)\n",
    "    result_scalars_test_images.append(scalar_r_this_image)\n",
    "\n",
    "\n",
    "    # --- Calcular scores de clasificación usando matrices POR IDENTIDAD ---\n",
    "    # Para cada imagen, calculamos un score para cada clase (0-9)\n",
    "\n",
    "    scores_c_for_image = [] # Scores usando matrices de Contenido por clase\n",
    "    scores_r_for_image = [] # Scores usando matrices de Resultado por clase\n",
    "\n",
    "    # Iterar sobre cada clase (0 a 9) para calcular el score con la imagen actual\n",
    "    for class_id in range(num_classes):\n",
    "        # Obtener los vectores aplanados de las matrices de Contenido y Resultado para ESTA clase\n",
    "        class_content_vector = flattened_content_vectors_per_class[class_id]\n",
    "        class_result_vector = flattened_result_vectors_per_class[class_id]\n",
    "\n",
    "        # Calcular el score: dot product entre el vector de interacción de la imagen\n",
    "        # y el vector aplanado de la matriz de la clase.\n",
    "        # Asegurarnos de que los vectores aplanados de clase no tienen NaN/Inf antes del dot product\n",
    "        # (Esto debería haber sido manejado en compute_matrices_from_vector si es necesario,\n",
    "        # o nan_to_num aplicado a flattened_content_vectors_per_class/result_vectors_per_class\n",
    "        # después de la sección 9). Agregaremos nan_to_num por seguridad aquí.\n",
    "        class_content_vector_finite = np.nan_to_num(class_content_vector, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        class_result_vector_finite = np.nan_to_num(class_result_vector, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        image_interaction_vector_finite = np.nan_to_num(flattened_image_interaction_vector, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "\n",
    "        score_c_k = np.dot(image_interaction_vector_finite, class_content_vector_finite)\n",
    "        score_r_k = np.dot(image_interaction_vector_finite, class_result_vector_finite)\n",
    "\n",
    "        scores_c_for_image.append(score_c_k)\n",
    "        scores_r_for_image.append(score_r_k)\n",
    "\n",
    "    # --- Obtener la clase predicha para esta imagen ---\n",
    "    # La clase predicha es aquella con el score más alto\n",
    "    predicted_class_c = np.argmax(scores_c_for_image)\n",
    "    predicted_class_r = np.argmax(scores_r_for_image)\n",
    "\n",
    "    # Añadir predicciones a las listas\n",
    "    predictions_c.append(predicted_class_c)\n",
    "    predictions_r.append(predicted_class_r)\n",
    "\n",
    "\n",
    "predictions_c = np.array(predictions_c)\n",
    "predictions_r = np.array(predictions_r)\n",
    "result_scalars_test_images = np.array(result_scalars_test_images) # Convertir a array numpy\n",
    "\n",
    "print(\"Procesamiento del conjunto de prueba completado.\")\n",
    "# print(f\"Predicciones Contenido (primeras 10): {predictions_c[:10]}\")\n",
    "# print(f\"Predicciones Resultado (primeras 10): {predictions_r[:10]}\")\n",
    "# print(f\"Etiquetas verdaderas (primeras 10): {y_test[:10]}\")\n",
    "\n",
    "\n",
    "# 11. Calcular y mostrar los porcentajes de acierto\n",
    "print(\"\\n--- Porcentajes de Acierto en el Conjunto de Prueba ---\")\n",
    "\n",
    "# Calcular precisión comparando las predicciones con las etiquetas verdaderas del conjunto de prueba\n",
    "accuracy_c = accuracy_score(y_test, predictions_c) * 100\n",
    "accuracy_r = accuracy_score(y_test, predictions_r) * 100\n",
    "\n",
    "print(f\"Acierto usando Matrices de Contenido (por identidad): {accuracy_c:.2f}%\")\n",
    "print(f\"Acierto usando Matrices de Resultado (por identidad): {accuracy_r:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f29ed686-f79d-43b1-8caf-b85a4b58fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Escalar Derivado de la Matriz de Resultado (por imagen de prueba) ---\n",
      "Valor escalar promedio derivado de la Matriz de Resultado (calculada por imagen de prueba): 462611843154.6099\n",
      "\n",
      "El escalar se obtiene sumando los valores absolutos de la matriz Resultado.\n",
      "La Matriz de Resultado se calcula para cada imagen de prueba usando su propio embedding.\n"
     ]
    }
   ],
   "source": [
    "# 12. Mostrar un escalar derivado de la matriz de Resultado\n",
    "print(\"\\n--- Escalar Derivado de la Matriz de Resultado (por imagen de prueba) ---\")\n",
    "\n",
    "# Calculamos la media de los escalares obtenidos para cada imagen de prueba\n",
    "# Excluimos NaN o Inf si los hay al calcular la media\n",
    "average_result_scalar_per_image = np.mean(result_scalars_test_images[np.isfinite(result_scalars_test_images)])\n",
    "\n",
    "print(f\"Valor escalar promedio derivado de la Matriz de Resultado (calculada por imagen de prueba): {average_result_scalar_per_image:.4f}\")\n",
    "print(\"\\nEl escalar se obtiene sumando los valores absolutos de la matriz Resultado.\")\n",
    "print(\"La Matriz de Resultado se calcula para cada imagen de prueba usando su propio embedding.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
